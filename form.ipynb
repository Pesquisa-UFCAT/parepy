{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from parepy_toolbox.distributions import non_normal_approach_normal\n",
    "\n",
    "# params_normal = {'loc': 0, 'scale': 1}\n",
    "# result_normal = non_normal_approach_normal(1.3065, 'normal', params_normal)\n",
    "# print(result_normal)\n",
    "\n",
    "# params_gumbel_max = {'loc': 0.887487, 'scale': 5.1302}\n",
    "# result_gumbel_max = non_normal_approach_normal(1.3065, 'gumbel max', params_gumbel_max)\n",
    "# print(result_gumbel_max)\n",
    "\n",
    "# params_log_normal = {'loc': 0.887487, 'scale': 5.1302}\n",
    "# result_log_normal = non_normal_approach_normal(1.3065, 'log normal', params_log_normal)\n",
    "# print(result_log_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj(x, none_variable=None):\n",
    "    simga_y = x[0]\n",
    "    w = x[1]\n",
    "    m = x[2]\n",
    "    return simga_y * w - m\n",
    "\n",
    "def grad_obj(x, none_variable=None):\n",
    "    sigma_y = x[0]\n",
    "    w = x[1]\n",
    "    m = x[2]\n",
    "    grad_g = [w, sigma_y, -1]\n",
    "    return grad_g\n",
    "\n",
    "def f_alpha(alpha,args):\n",
    "    xk,d=args[0],args[1]\n",
    "    xaux=xk+alpha*d\n",
    "    f=obj(xaux)\n",
    "    return f\n",
    "\n",
    "# # Dataset\n",
    "# f = {'type': 'normal', 'parameters': {'mean': 40, 'sigma': 5.}, 'stochastic variable': False}\n",
    "# p = {'type': 'normal', 'parameters': {'mean': 50, 'sigma': 2.5}, 'stochastic variable': False}\n",
    "# w = {'type': 'normal', 'parameters': {'mean': 1000, 'sigma': 200}, 'stochastic variable': False}\n",
    "# var = [f, p, w]\n",
    "\n",
    "# # PAREpy setup\n",
    "# setup = {\n",
    "#                 'tolerance': 1e-6, \n",
    "#                 'max iterations': 20,\n",
    "#                 'numerical model': 'fosm', \n",
    "#                 'variables settings': var, \n",
    "#                 'number of state limit functions or constraints': 1, \n",
    "#                 'none variable': None,\n",
    "#                 'objective function': obj,\n",
    "#                 'gradient objective function': grad_obj,\n",
    "#                 'name simulation': None,\n",
    "#         }\n",
    "\n",
    "def deterministic_algorithm_structural_analysis(setup: dict) -> tuple[float, int]:\n",
    "    try:\n",
    "        # Verifica se o setup é um dicionário\n",
    "        if not isinstance(setup, dict):\n",
    "            raise TypeError('The setup parameter must be a dictionary.')\n",
    "        \n",
    "        # Verifica se as chaves obrigatórias estão presentes\n",
    "        required_keys = [\n",
    "            'tolerance', 'max iterations', 'numerical model', 'variables settings',\n",
    "            'number of state limit functions or constraints', 'none variable',\n",
    "            'objective function', 'gradient objective function', 'name simulation'\n",
    "        ]\n",
    "        for key in required_keys:\n",
    "            if key not in setup:\n",
    "                raise ValueError(f'The setup parameter must have the key: {key}.')\n",
    "        \n",
    "        # Extrai as variáveis e constrói o dicionário no formato especificado\n",
    "        variables = setup['variables settings']\n",
    "        params_adapt  = {}  # Dicionário para armazenar as variáveis convertidas\n",
    "        \n",
    "        for i, var in enumerate(variables):\n",
    "            if not isinstance(var, dict):\n",
    "                raise TypeError('Each variable in \"variables settings\" must be a dictionary.')\n",
    "            \n",
    "            if 'parameters' not in var or not isinstance(var['parameters'], dict):\n",
    "                raise ValueError('Each variable must have a \"parameters\" key with a dictionary value.')\n",
    "            \n",
    "            if 'mean' not in var['parameters'] or 'sigma' not in var['parameters']:\n",
    "                raise ValueError('Each variable must have \"mean\" and \"sigma\" in its parameters.')\n",
    "            \n",
    "            mean = var['parameters']['mean']\n",
    "            std = var['parameters']['sigma']\n",
    "            \n",
    "            if var['type'] == 'normal':\n",
    "                params_adapt[f'var{i}'] = {\n",
    "                    'type': 'normal',\n",
    "                    'params': {\n",
    "                        'mu': mean,\n",
    "                        'sigma': std\n",
    "                    }\n",
    "                }\n",
    "            elif var['type'] == 'lognormal':\n",
    "                epsilon = np.sqrt(np.log(1 + (std / mean) ** 2))\n",
    "                lambdaa = np.log(mean) - 0.5 * epsilon ** 2\n",
    "                params_adapt[f'var{i}'] = {\n",
    "                    'type': 'lognormal',\n",
    "                    'params': {\n",
    "                        'lambda': lambdaa,\n",
    "                        'epsilon': epsilon\n",
    "                    }\n",
    "                }\n",
    "            elif var['type'] == 'gumbel max':\n",
    "                gamma = 0.577215665  \n",
    "                beta = np.pi / (np.sqrt(6) * std)\n",
    "                calpha = mean - gamma / beta\n",
    "                params_adapt[f'var{i}'] = {\n",
    "                    'type': 'gumbel max',\n",
    "                    'params': {\n",
    "                        'alpha': calpha,\n",
    "                        'beta': beta\n",
    "                    }\n",
    "                }\n",
    "            elif var['type'] == 'gumbel min':\n",
    "                gamma = 0.577215665 \n",
    "                beta = np.pi / (np.sqrt(6) * std)\n",
    "                alpha = mean + gamma / beta\n",
    "                params_adapt[f'var{i}'] = {\n",
    "                    'type': 'gumbel min',\n",
    "                    'params': {\n",
    "                        'alpha': alpha,\n",
    "                        'beta': beta\n",
    "                    }\n",
    "                }\n",
    "                 \n",
    "        tol = setup['tolerance']\n",
    "        max_iter = setup['max iterations']\n",
    "        none_variable = setup['none variable']\n",
    "        obj = setup['objective function']\n",
    "        grad_obj = setup['gradient objective function']\n",
    "\n",
    "        for index, value in params_adapt.items():\n",
    "            print(f\"index: {index}, \\nvalue: {value}\")\n",
    "\n",
    "        print(f\"mu: {mean}, \\nsigma: {std}, \\ntol: {tol}, \\nmax_iter: {max_iter}, \\nnone_variable: {none_variable}\")\n",
    "        \n",
    "        # Fixed in this algorithm\n",
    "        beta_list = [10000]\n",
    "        error = 1000\n",
    "        iter = 0\n",
    "        step = 1\n",
    "\n",
    "        x = np.transpose(np.array([mu.copy()]))\n",
    "        mu = x.copy()\n",
    "        jacobian_xy = np.diag(std)\n",
    "        jacobian_xy_trans = np.transpose(jacobian_xy)\n",
    "        jacobian_yx = np.linalg.inv(jacobian_xy)\n",
    "        y = jacobian_yx @ (x - mu)\n",
    "        x = jacobian_xy @ y + mu\n",
    "\n",
    "        while (error > tol and iter < max_iter):\n",
    "            beta = np.linalg.norm(y)\n",
    "            beta_list.append(beta)\n",
    "            g_y = obj(x.flatten().tolist())\n",
    "            grad_g_x = grad_obj(x.flatten().tolist())\n",
    "            grad_g_y = np.dot(jacobian_xy_trans, np.transpose(np.array([grad_g_x])))\n",
    "            num = (np.transpose(grad_g_y) @ y - g_y)\n",
    "            norm = np.linalg.norm(grad_g_y)\n",
    "            norm2 = norm ** 2\n",
    "            #alpha = grad_g_y / norm\n",
    "            #aux = g_y / norm\n",
    "            #y = -alpha * (beta + aux)\n",
    "            d = grad_g_y @ (num / norm2) - y\n",
    "            #step = minimize_scalar(f_alpha, bounds=(.001, 1), args=([y, d]), method='bounded')\n",
    "            #print(step.x)\n",
    "            #y += step.x * d\n",
    "            y += step * d\n",
    "            error = np.abs(beta_list[iter + 1] - beta_list[iter])\n",
    "            x = jacobian_xy @ y + mu\n",
    "            iter += 1\n",
    "        return float(beta), iter\n",
    "\n",
    "    except TypeError as e:\n",
    "        print(f\"TypeError: {e}\")\n",
    "        return None, iter\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Exception: {e}\")\n",
    "            return None, iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index: var0, \n",
      "value: {'type': 'lognormal', 'params': {'lambda': np.float64(3.6811273608459536), 'epsilon': np.float64(0.1245158083777528)}}\n",
      "index: var1, \n",
      "value: {'type': 'normal', 'params': {'mu': 50, 'sigma': 2.5}}\n",
      "index: var2, \n",
      "value: {'type': 'normal', 'params': {'mu': 1000, 'sigma': 200}}\n",
      "mu: 1000, \n",
      "sigma: 200, \n",
      "tol: 1e-06, \n",
      "max_iter: 20, \n",
      "none_variable: None\n",
      "Exception: cannot access local variable 'mu' where it is not associated with a value\n",
      "(None, 0)\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "f = {'type': 'lognormal', 'parameters': {'mean': 40, 'sigma': 5.}, 'stochastic variable': False}\n",
    "p = {'type': 'normal', 'parameters': {'mean': 50, 'sigma': 2.5}, 'stochastic variable': False}\n",
    "w = {'type': 'normal', 'parameters': {'mean': 1000, 'sigma': 200}, 'stochastic variable': False}\n",
    "var = [f, p, w]\n",
    "\n",
    "# PAREpy setup\n",
    "setup = {\n",
    "                'tolerance': 1e-6, \n",
    "                'max iterations': 20,\n",
    "                'numerical model': 'fosm', \n",
    "                'variables settings': var, \n",
    "                'number of state limit functions or constraints': 1, \n",
    "                'none variable': None,\n",
    "                'objective function': obj,\n",
    "                'gradient objective function': grad_obj,\n",
    "                'name simulation': None,\n",
    "        }\n",
    "\n",
    "# Call algorithm\n",
    "beta = deterministic_algorithm_structural_analysis(setup)\n",
    "print(beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ambiente_parepy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
